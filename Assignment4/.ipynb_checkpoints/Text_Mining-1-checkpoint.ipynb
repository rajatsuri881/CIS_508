{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "w1hDk9hDvOH7",
    "outputId": "4f0038b7-1769-4a95-ef0b-1752f5e56b04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#NLTK-------------------------------\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stemporter import PorterStemmer\n",
    "\n",
    "# Import libraries for feature \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "#Change current working directory to gdrive\n",
    "%cd /gdrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BMwGQK7KAd7T",
    "outputId": "17e0b29c-d05a-4d49-85fe-6ee6a4388cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 2)\n",
      "(2070, 17)\n"
     ]
    }
   ],
   "source": [
    "#Read files\n",
    "textfile = r'/gdrive/My Drive/Comments.csv'\n",
    "textData = pd.read_csv(textfile) #creates a dataframe\n",
    "\n",
    "CustInfofile = r'/gdrive/My Drive/Customers.csv'\n",
    "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
    "\n",
    "print(textData.shape)\n",
    "print(CustInfoData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "SWOTk6C1Ao45",
    "outputId": "f19ba3df-af7b-43d7-89f6-2c7839d317d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 16)\n",
      "(2070, 2)\n",
      "0       Cancelled\n",
      "1         Current\n",
      "2         Current\n",
      "3         Current\n",
      "4       Cancelled\n",
      "          ...    \n",
      "2065    Cancelled\n",
      "2066    Cancelled\n",
      "2067    Cancelled\n",
      "2068    Cancelled\n",
      "2069    Cancelled\n",
      "Name: TARGET, Length: 2070, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Extract target column from Customer Info file\n",
    "y_train = CustInfoData[\"TARGET\"]\n",
    "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
    "                     \n",
    "print(X_train.shape)\n",
    "print(textData.shape)\n",
    "textData.head()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuWYNz2Ep17l"
   },
   "outputs": [],
   "source": [
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "#Tokenize - Split the sentences to lists of words\n",
    "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
    "\n",
    "#export_csv = textData.to_csv(r'/gdrive/My Drive/CIS508/TextDataTokenized1.csv')\n",
    "\n",
    "#Now do stemming - create a new dataframe to store stemmed version\n",
    "newTextData=pd.DataFrame()\n",
    "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
    "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "\n",
    "#export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/newTextDataTS.csv')\n",
    "\n",
    "#Join stemmed strings\n",
    "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/newTextData-Joined.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "WiBguQloljam",
    "outputId": "f0dbaba5-ef61-41c5-d18e-a8398fe68718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 354)\n",
      "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n"
     ]
    }
   ],
   "source": [
    "#Do Bag-Of-Words model - Term - Document Matrix\n",
    "#Learn the vocabulary dictionary and return term-document matrix.\n",
    "#count_vect = CountVectorizer(stop_words=None)\n",
    "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
    "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
    "print(TD_counts.shape)\n",
    "TD_counts.dtype\n",
    "print(count_vect.get_feature_names())\n",
    "#print(TD_counts)\n",
    "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
    "#print(DF_TD_Counts)\n",
    "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/CIS508/TD_counts-TokenizedStemmed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pd8TZYnAxQbP",
    "outputId": "bdd9ac63-8bc9-4032-ff3a-963ee7a6356f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 354)\n",
      "      0    1    2    3        4    5    ...  348  349  350  351  352  353\n",
      "0     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1     0.0  0.0  0.0  0.0  0.27568  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...   ...  ...  ...  ...      ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "2065  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2066  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2067  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2068  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2069  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 354 columns]\n"
     ]
    }
   ],
   "source": [
    "#Compute TF-IDF Matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "DF_TF_IDF=pd.DataFrame(X_train_tfidf.toarray())\n",
    "print(DF_TF_IDF)\n",
    "export_csv= DF_TF_IDF.to_csv(r'/gdrive/My Drive/CIS508/TFIDF_counts-TokenizedStemmed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "m2owIUD6_eYO",
    "outputId": "771b3bdc-121c-4f8b-f3e1-76edb1677ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0    1    2    3         4         5   ...   44   45   46   47   48   49\n",
      "0     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4     0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...        ...  ...  ...  ...       ...       ...  ...  ...  ...  ...  ...  ...  ...\n",
      "2065  0.000000  0.0  0.0  0.0  0.000000  0.446161  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2066  0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2067  0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2068  0.772949  0.0  0.0  0.0  0.545354  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2069  0.000000  0.0  0.0  0.0  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "#Feature selection\n",
    "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=50).fit_transform(DF_TF_IDF,y_train)\n",
    "new_DF_TF_IDF.shape\n",
    "\n",
    "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
    "print(DF_TF_IDF_SelectedFeatures)\n",
    "\n",
    "export_csv= DF_TF_IDF_SelectedFeatures.to_csv(r'/gdrive/My Drive/CIS508/TFIDF_counts-Selected Features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "geVCLka8xxjf",
    "outputId": "2447578b-b8c2-4f1d-a2af-36f4ce23df33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (training): 0.632850\n",
      "Confusion Matrix:\n",
      "[[  86  718]\n",
      " [  42 1224]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Cancelled       0.67      0.11      0.18       804\n",
      "     Current       0.63      0.97      0.76      1266\n",
      "\n",
      "    accuracy                           0.63      2070\n",
      "   macro avg       0.65      0.54      0.47      2070\n",
      "weighted avg       0.65      0.63      0.54      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Construct a Random Forest Classifier on text data\n",
    "clf=RandomForestClassifier()\n",
    "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y_train)\n",
    "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures, y_train)))\n",
    "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, rf_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_train, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "Fq_7eeQOz7xk",
    "outputId": "dd3c9bdd-b7f6-4d23-c717-759a2a8bbc57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 17)\n",
      "(2070, 16)\n",
      "(2070, 66)\n",
      "        ID Sex Status  Children  Est_Income  ...   45   46   47   48   49\n",
      "0        1   F      S         1    38000.00  ...  0.0  0.0  0.0  0.0  0.0\n",
      "1        6   M      M         2    29616.00  ...  0.0  0.0  0.0  0.0  0.0\n",
      "2        8   M      M         0    19732.80  ...  0.0  0.0  0.0  0.0  0.0\n",
      "3       11   M      S         2       96.33  ...  0.0  0.0  0.0  0.0  0.0\n",
      "4       14   F      M         2    52004.80  ...  0.0  0.0  0.0  0.0  0.0\n",
      "...    ...  ..    ...       ...         ...  ...  ...  ...  ...  ...  ...\n",
      "2065  3821   F      S         0    78851.30  ...  0.0  0.0  0.0  0.0  0.0\n",
      "2066  3822   F      S         1    17540.70  ...  0.0  0.0  0.0  0.0  0.0\n",
      "2067  3823   F      M         0    83891.90  ...  0.0  0.0  0.0  0.0  0.0\n",
      "2068  3824   F      M         2    28220.80  ...  0.0  0.0  0.0  0.0  0.0\n",
      "2069  3825   F      S         0    28589.10  ...  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "#Merge files\n",
    "\n",
    "print(CustInfoData.shape)\n",
    "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
    "print(X_train.shape)\n",
    "combined=pd.concat([X_train, DF_TF_IDF_SelectedFeatures], axis=1)\n",
    "print(combined.shape)\n",
    "print(combined)\n",
    "export_csv= combined.to_csv(r'/gdrive/My Drive/CIS508/Combined-Cust+TFIDF+SelectedFeatures.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EPmJYiWrPkRG",
    "outputId": "700a2c31-55b5-4fa1-8b26-df64f357759d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype']\n",
      "(2070, 74)\n"
     ]
    }
   ],
   "source": [
    "#Do one Hot encoding for categorical features\n",
    "X_cat = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
    "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
    "print(X_cat)\n",
    "combined_one_hot = pd.get_dummies(combined,columns=X_cat)\n",
    "print(combined_one_hot.shape)\n",
    "export_csv= combined_one_hot.to_csv(r'/gdrive/My Drive/CIS508/combined_one_hot.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "DEMpFKQAVuV1",
    "outputId": "6100ef5a-6ddd-47b4-96d9-a7ce47ed59e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (training): 0.988889\n",
      "Confusion Matrix:\n",
      "[[ 795    9]\n",
      " [  14 1252]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Cancelled       0.98      0.99      0.99       804\n",
      "     Current       0.99      0.99      0.99      1266\n",
      "\n",
      "    accuracy                           0.99      2070\n",
      "   macro avg       0.99      0.99      0.99      2070\n",
      "weighted avg       0.99      0.99      0.99      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Construct a Random Forest Classifier on combined data\n",
    "#clf1=RandomForestClassifier()\n",
    "RF_Comb = clf.fit(combined_one_hot,y_train)\n",
    "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(combined_one_hot, y_train)))\n",
    "rf_predictions = clf.predict(combined_one_hot)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, rf_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_train, rf_predictions))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
